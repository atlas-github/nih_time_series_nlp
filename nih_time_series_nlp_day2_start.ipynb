{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmbqrBYsXTg+UpTj1BhV8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/nih_time_series_nlp/blob/main/nih_time_series_nlp_day2_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09:00 am: Feature engineering for time series data"
      ],
      "metadata": {
        "id": "LfQLP03iDgIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Lag Features\n",
        "Lag features are previous values of the time series that can help capture temporal dependencies."
      ],
      "metadata": {
        "id": "hVnyFbPmELjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'date': pd.date_range(start='2024-01-01', periods=10, freq='D'),\n",
        "    'value': [10, 12, 15, 14, 16, 18, 20, 22, 21, 23]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "# create lag features\n",
        "# refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html\n"
      ],
      "metadata": {
        "id": "ShK4Qlp6ENBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rolling Statistics (Moving Average)\n",
        "\n",
        "Rolling statistics help in smoothing the time series data and identifying trends."
      ],
      "metadata": {
        "id": "82tvbbFeENJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# moving average with a window of 3 days\n",
        "# refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html\n"
      ],
      "metadata": {
        "id": "o-IJsA6FENPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourier Transform\n",
        "Fourier Transform helps in analyzing the frequency components of the time series."
      ],
      "metadata": {
        "id": "YqqZBKUDWUke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate sample data\n",
        "# refer to https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html\n",
        "\n",
        "\n",
        "# plot the Fourier Transform\n",
        "\n"
      ],
      "metadata": {
        "id": "d7VDPGVrWUpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exponential Moving Average\n",
        "\n",
        "The Exponential Moving Average (EMA) gives more weight to recent observations."
      ],
      "metadata": {
        "id": "purBNgmlWUvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exponential moving average with a span of 3 days\n",
        "# refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n"
      ],
      "metadata": {
        "id": "IdRkmdllWUzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09:45 am: Time series data normalization and scaling"
      ],
      "metadata": {
        "id": "iocjqNabDgLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "Normalization (or Min-Max Scaling) scales the data to a fixed range, usually [0, 1]."
      ],
      "metadata": {
        "id": "xxjXAmHuEN2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'date': pd.date_range(start='2024-01-01', periods=10, freq='D'),\n",
        "    'value': [10, 12, 15, 14, 16, 18, 20, 22, 21, 23]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "# normalizing\n",
        "# refer to https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n"
      ],
      "metadata": {
        "id": "FMXCxPV0EN8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization\n",
        "Standardization (or Z-score normalization) scales the data so that it has a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "VvWEBVdKEOCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# standardizing\n",
        "# refer to https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n"
      ],
      "metadata": {
        "id": "FD0kxPIlEOIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effects of Scaling on Time Series Analysis\n",
        "Scaling can impact time series models and their predictions. Let’s compare how normalization and standardization affect a simple moving average."
      ],
      "metadata": {
        "id": "uCRiPN23W4Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compute moving averages before scaling\n",
        "\n",
        "\n",
        "# compute moving averages after normalization\n",
        "\n",
        "\n",
        "# compute moving averages after standardization\n",
        "\n",
        "\n",
        "# plot\n"
      ],
      "metadata": {
        "id": "Nzm4V11nW4N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11:00 am: Practical session 4\n"
      ],
      "metadata": {
        "id": "BXAKoPoKDgNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the gdown library\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "eMXS1YtCEO4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece64954-ce3d-4518-dccd-cb89ce1131ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download dengue csv\n",
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "# insert file_id\n",
        "file_id = '1F-faNnQoyhdjbuyEVZPHV1cv_h0h5UDl'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# download the CSV file\n",
        "gdown.download(url, 'dengue.csv', quiet=False)\n",
        "\n",
        "# read the csv file into a DataFrame\n",
        "df_dengue = pd.read_csv('dengue.csv')\n",
        "df_dengue"
      ],
      "metadata": {
        "id": "sJ5gbTzHhKh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter to only needed columns\n",
        "df_dengue_filtered = df_dengue[[\"NO_KES\", \"TRK_NOTI\", \"EPID_MINGG\", \"EPID_TAHUN\", \"JNS_KES\", \"NO_RUMAH\",\n",
        "                                \"POSKOD\", \"LOKALITI\", \"MUKIM\", \"DAERAH\", \"NEGERI\", \"LATITUDE\", \"LONGITUDE\", \"PENTADBIRA\",\n",
        "                                \"WARGANEGAR\", \"STATUS_PEN\", \"STATUS_LOK\"]]\n",
        "# convert to datetime\n",
        "df_dengue_filtered['TRK_NOTI'] = pd.to_datetime(df_dengue_filtered['TRK_NOTI'], format = \"%Y/%m/%d\")\n",
        "\n",
        "# which postcodes have most complete vs. most missing data?\n",
        "postcode_counts = df_dengue_filtered['POSKOD'].value_counts().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "postcode_counts.columns = ['postcode', 'count']\n",
        "\n",
        "postcode_counts"
      ],
      "metadata": {
        "id": "JO8C8hMVhKkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter to only postcode 47620\n"
      ],
      "metadata": {
        "id": "SR19E1eYQ9AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset index\n"
      ],
      "metadata": {
        "id": "stIcwo7gQ9C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pivot table"
      ],
      "metadata": {
        "id": "4W8GveZ_Q9Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset index to make 'date' a column again\n",
        "\n",
        "\n",
        "# melt the DataFrame to long format\n",
        "\n",
        "\n",
        "# extract the case type from the 'case_type' column\n"
      ],
      "metadata": {
        "id": "A_imp06ZhMRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter to only wabak\n"
      ],
      "metadata": {
        "id": "f9m6VLNVEPDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# sample data\n",
        "\n",
        "\n",
        "# create lag features\n"
      ],
      "metadata": {
        "id": "_wX2pi2CR42Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # reset index\n"
      ],
      "metadata": {
        "id": "Sy3SnJ_5Sfg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the plot\n",
        "\n",
        "\n",
        "# customize the plot\n",
        "\n",
        "\n",
        "\n",
        "# show the plot\n"
      ],
      "metadata": {
        "id": "Pa0vXkMKR44f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve external data example: https://open-meteo.com/en/docs/historical-weather-api#start_date=2014-12-28&end_date=2022-02-06&hourly=&daily=rain_sum&timezone=Asia%2FSingapore\n"
      ],
      "metadata": {
        "id": "n-h3M2IIR469"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to datetime\n"
      ],
      "metadata": {
        "id": "kAY14kz8V-1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# create the main figure and axis\n",
        "\n",
        "\n",
        "# first plot: Dengue case counts and lag_2 on the primary y-axis\n",
        "\n",
        "\n",
        "# customize the first axis\n",
        "\n",
        "\n",
        "# create a secondary y-axis for rainfall\n",
        "\n",
        "\n",
        "# second plot: Daily rainfall on the secondary y-axis\n",
        "\n",
        "\n",
        "# customize the second axis\n",
        "\n",
        "\n",
        "# adjust layout\n",
        "\n",
        "\n",
        "# show the plot\n"
      ],
      "metadata": {
        "id": "vatvlY_IR49Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXKgkSprR4_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12:00 pm: Time series decomposition"
      ],
      "metadata": {
        "id": "efnurqucDgPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series decomposition is a useful technique for analyzing the components of a time series. It typically breaks down a time series into trend, seasonality, and residuals (or noise). There are two main types of decomposition: additive and multiplicative."
      ],
      "metadata": {
        "id": "BYD5LsOrXLcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additive Decomposition\n",
        "In additive decomposition, the time series is assumed to be the sum of the trend, seasonality, and residuals."
      ],
      "metadata": {
        "id": "Ro6-kImMEPjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# generate sample data\n",
        "np.random.seed(0)\n",
        "dates = pd.date_range(start='2024-01-01', periods=100, freq='D')\n",
        "trend = np.linspace(10, 20, 100)\n",
        "\n",
        "\n",
        "# create DataFrame\n",
        "\n",
        "\n",
        "# perform additive decomposition\n",
        "\n",
        "# plotting the components\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pTAiFa6tEPqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplicative Decomposition\n",
        "In multiplicative decomposition, the time series is assumed to be the product of the trend, seasonality, and residuals."
      ],
      "metadata": {
        "id": "Mx7wToDNEPvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform multiplicative decomposition\n",
        "\n",
        "\n",
        "# plot the components\n"
      ],
      "metadata": {
        "id": "DFEpJI4uEP0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Additive vs. Multiplicative Decomposition\n",
        "It's useful to compare how the decompositions differ, especially if the data exhibits multiplicative seasonality or varying amplitude."
      ],
      "metadata": {
        "id": "BF5-l-65XdQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate sample data with multiplicative seasonality\n",
        "\n",
        "\n",
        "# create DataFrame for multiplicative example\n",
        "\n",
        "\n",
        "# perform decompositions\n",
        "\n",
        "\n",
        "# plot the results for both decompositions\n",
        "\n"
      ],
      "metadata": {
        "id": "DoqF6aP1XdWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2:00 pm: Practical session 5"
      ],
      "metadata": {
        "id": "sb0KR3wdDgSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11 classical time series forecasting methods in Python (article link [here](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/))"
      ],
      "metadata": {
        "id": "2ioQOjbpFFNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autoregressive (AR) Model\n",
        "The autoregression (AR) method predicts the subsequent value in a sequence using a linear combination of previous observations.\n",
        "\n",
        "The notation for the model involves specifying the order of the model p as a parameter to the AR function, e.g. AR(p). For example, AR(1) is a first-order autoregression model.\n",
        "\n",
        "The method is best suited for single-variable time series that lack trend and seasonal components."
      ],
      "metadata": {
        "id": "1M3Z-YumHNSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AR example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n"
      ],
      "metadata": {
        "id": "U1gyrXh6EQif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moving Average (MA) Model\n",
        "The Moving Average (MA) method models predict the next step in the sequence as a linear function of the residual errors from a mean process at prior time steps.\n",
        "\n",
        "It’s important to note that a Moving Average model is different from calculating the moving average of the time series.\n",
        "\n",
        "The notation for the model involves specifying the order of the model q as a parameter to the MA function, e.g. MA(q). For example, MA(1) is a first-order moving average model.\n",
        "\n",
        "The method is suitable for univariate time series without trend and seasonal components.\n",
        "\n",
        "We can use the ARIMA class to create an MA model and set a zeroth-order AR model. We must specify the order of the MA model in the order argument."
      ],
      "metadata": {
        "id": "6mBS4QD8EQoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MA example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "JAQ3H-zYEQt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autoregressive Moving Average (ARMA)\n",
        "The Autoregressive Moving Average (ARMA) method model predicts the next step in the sequence based on a linear combination of both past observations and past residual errors.\n",
        "\n",
        "The method combines both Autoregression (AR) and Moving Average (MA) models.\n",
        "\n",
        "To represent the model, the notation involves specifying the order for the AR(p) and MA(q) models as parameters to an ARMA function, e.g. ARMA(p, q). An ARIMA model can be used to develop AR or MA models.\n",
        "\n",
        "The method is suitable for univariate time series without trend and seasonal components."
      ],
      "metadata": {
        "id": "6FGSJia3GFYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARMA example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "UvbLTYu9GFd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autoregressive Integrated Moving Average (ARIMA)\n",
        "\n",
        "The Autoregressive Integrated Moving Average (ARIMA) method model predicts the next step in the sequence as a linear function of the differenced observations and residual errors at prior time steps.\n",
        "\n",
        "The method integrates the principles of Autoregression (AR) and Moving Average (MA) models as well as a differencing pre-processing step of the sequence to make the sequence stationary, called integration (I).\n",
        "\n",
        "The notation for the model involves specifying the order for the AR(p), I(d), and MA(q) models as parameters to an ARIMA function, e.g. ARIMA(p, d, q). An ARIMA model can also be used to develop AR, MA, and ARMA models.\n",
        "\n",
        "The ARIMA approach is optimal for single-variable time series that exhibit a trend but lack seasonal variations."
      ],
      "metadata": {
        "id": "TuqMKjTzGFil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "n-SupKnlGFnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
        "\n",
        "The Seasonal Autoregressive Integrated Moving Average (SARIMA) method models the next step in the sequence based on a linear blend of differenced observations, errors, differenced seasonal observations, and seasonal errors at prior time steps.\n",
        "\n",
        "SARIMA enhances the ARIMA model with the ability to perform the same autoregression, differencing, and moving average modeling at the seasonal level.\n",
        "\n",
        "The notation for the model involves specifying the order for the AR(p), I(d), and MA(q) models as parameters to an ARIMA function and AR(P), I(D), MA(Q) and m parameters at the seasonal level, e.g. SARIMA(p, d, q)(P, D, Q)m where “m” is the number of time steps in each season (the seasonal period). A SARIMA model can be used to develop AR, MA, ARMA and ARIMA models.\n",
        "\n",
        "The method is suitable for univariate time series with trend and/or seasonal components."
      ],
      "metadata": {
        "id": "jMlja2tAHgoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SARIMA example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "s5u7obSTHgtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\n",
        "\n",
        "The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables.\n",
        "\n",
        "Exogenous variables are also called covariates and can be thought of as parallel input sequences that have observations at the same time steps as the original series. The primary series may be referred to as endogenous data to contrast it from the exogenous sequence(s). The observations for exogenous variables are included in the model directly at each time step and are not modeled in the same way as the primary endogenous sequence (e.g. as an AR, MA, etc. process).\n",
        "\n",
        "The SARIMAX method can also be used to model the subsumed models with exogenous variables, such as ARX, MAX, ARMAX, and ARIMAX.\n",
        "\n",
        "The method is suitable for univariate time series with trend and/or seasonal components and exogenous variables."
      ],
      "metadata": {
        "id": "nfunXoe7HgxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SARIMAX example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "ogZTQ04gHg1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Autoregression (VAR)"
      ],
      "metadata": {
        "id": "HEE_maLRHg6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contrived dataset with dependency\n",
        "\n",
        "\n",
        "# convert to NumPy array\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "# make prediction\n",
        "\n",
        "\n",
        "# pass the last known observations as input for forecasting\n",
        "\n",
        "# get the number of lags used in the model\n",
        "\n",
        "\n",
        "# Get the last 'lag_order' observations\n"
      ],
      "metadata": {
        "id": "PigckXkyHg-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Autoregression Moving-Average (VARMA)\n",
        "\n",
        "The Vector Autoregression Moving-Average (VARMA) method models the upcoming value in multiple time series by utilising the ARMA model approach. It is the generalization of ARMA to multiple parallel time series, e.g. multivariate time series.\n",
        "\n",
        "The notation for the model involves specifying the order for the AR(p) and MA(q) models as parameters to a VARMA function, e.g. VARMA(p, q). A VARMA model can also be used to develop VAR or VMA models.\n",
        "\n",
        "The method is suitable for multivariate time series without trend and seasonal components."
      ],
      "metadata": {
        "id": "hv9ZAnBQHhB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VARMA example\n",
        "\n",
        "\n",
        "# contrived dataset with dependency\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "o0gKSoICIOrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\n",
        "\n",
        "The Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX) extends the capabilities of the VARMA model which also includes the modelling of exogenous variables. It is a multivariate version of the ARMAX method.\n",
        "\n",
        "Exogenous variables, also called covariates and can be thought of as parallel input sequences that align with the time steps as the original series. The primary series(es) are referred to as endogenous data to contrast it from the exogenous sequence(s). The observations for exogenous variables are included in the model directly at each time step and are not modeled in the same way as the primary endogenous sequence (e.g. as an AR, MA, etc. process).\n",
        "\n",
        "The VARMAX method can also be used to model the subsumed models with exogenous variables, such as VARX and VMAX.\n",
        "\n",
        "The method is suitable for multivariate time series without trend and seasonal components with exogenous variables."
      ],
      "metadata": {
        "id": "KxlPL_3tIOyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VARMAX example\n",
        "\n",
        "\n",
        "# contrived dataset with dependency\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n"
      ],
      "metadata": {
        "id": "MB4on--6IO3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple Exponential Smoothing (SES)\n",
        "\n",
        "The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function of observations at prior time steps.\n",
        "\n",
        "The method is suitable for univariate time series without trend and seasonal components."
      ],
      "metadata": {
        "id": "lTxH8LgCIhkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SES example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "0YfA_RgvIhBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Holt Winter’s Exponential Smoothing (HWES)"
      ],
      "metadata": {
        "id": "zPYb08IyIh_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HWES example\n",
        "\n",
        "\n",
        "# contrived dataset\n",
        "\n",
        "\n",
        "# fit model\n",
        "\n",
        "\n",
        "# make prediction\n"
      ],
      "metadata": {
        "id": "mBswci_nIhDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link to [Prophet](https://facebook.github.io/prophet/)"
      ],
      "metadata": {
        "id": "BqTS0iTSI0uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install prophet library\n",
        "!pip install prophet"
      ],
      "metadata": {
        "id": "phPkVLsEI0zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate sample data\n",
        "\n",
        "\n",
        "# plot original data\n"
      ],
      "metadata": {
        "id": "lTaaB-C8I1AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and fit Prophet model\n"
      ],
      "metadata": {
        "id": "hpm6-N-9JByC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe for future dates\n",
        "\n",
        "\n",
        "# make predictions\n",
        "\n",
        "\n",
        "# plot the forecast\n"
      ],
      "metadata": {
        "id": "uyGXjvXlJB0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add custom seasonality\n"
      ],
      "metadata": {
        "id": "UGISdTo4JB27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define holidays\n",
        "\n",
        "\n",
        "# add holidays to the model\n",
        "\n",
        "\n",
        "# create future dataframe and make predictions\n",
        "\n",
        "\n",
        "\n",
        "# Plot forecast with holidays\n"
      ],
      "metadata": {
        "id": "TQw7gMK2JB5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# introduce missing data\n",
        "\n",
        "\n",
        "# introduce outliers\n",
        "\n"
      ],
      "metadata": {
        "id": "XP5dNm7mJOxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit Prophet model with missing data\n",
        "\n",
        "\n",
        "# make predictions\n",
        "\n",
        "\n",
        "\n",
        "# plot\n"
      ],
      "metadata": {
        "id": "wAPnt-UYJO0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Basic Prophet Forecasting: Easy to use and fits well with time series data exhibiting seasonality and trends.\n",
        "2.   Custom Seasonality: Allows for the addition of custom seasonal effects.\n",
        "3.   Holidays: Can incorporate holiday effects to adjust forecasts.\n",
        "4.   Missing Data and Outliers: Prophet can handle missing data and outliers by filling in missing values or treating outliers as anomalies."
      ],
      "metadata": {
        "id": "ZVyiOaIHJWnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4:00 pm: Resampling and time series frequency conversion"
      ],
      "metadata": {
        "id": "HoHKpXVoDgXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downsampling\n",
        "Description: Downsampling involves reducing the frequency of your time series data. For instance, converting daily data to monthly data.\n",
        "\n",
        "Example: Downsampling daily data to monthly data."
      ],
      "metadata": {
        "id": "T5LeRqbNESIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate sample daily data\n",
        "np.random.seed(0)\n",
        "dates = pd.date_range(start='2024-01-01', periods=365, freq='D')\n",
        "data = np.random.randn(365)\n",
        "df = pd.DataFrame({'date': dates, 'value': data})\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "# downsample to monthly frequency\n",
        "\n",
        "\n",
        "# plot original and resampled data\n"
      ],
      "metadata": {
        "id": "geECV330EGOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "1.   Reduces the amount of data, which can simplify analysis.\n",
        "2.   Useful for identifying long-term trends.\n",
        "\n",
        "Cons:\n",
        "\n",
        "1.   Loss of detail due to aggregation.\n",
        "2.   May miss short-term variations."
      ],
      "metadata": {
        "id": "tiZbCwUyESzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upsampling\n",
        "Description: Upsampling involves increasing the frequency of your time series data. For example, converting monthly data to daily data.\n",
        "\n",
        "Example: Upsampling monthly data to daily data and filling in missing values."
      ],
      "metadata": {
        "id": "9hGHMAKEJ3NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate sample monthly data\n",
        "\n",
        "\n",
        "# upsample to daily frequency\n",
        "# forward fill to handle missing values\n",
        "\n",
        "# plot original and upsampled data\n"
      ],
      "metadata": {
        "id": "ger4ovRkES6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "1.  Increases data granularity.\n",
        "2.  Useful for detailed analysis and interpolating data.\n",
        "\n",
        "Cons:\n",
        "\n",
        "1.  May introduce artificial patterns.\n",
        "2.  Requires careful handling of missing values."
      ],
      "metadata": {
        "id": "8kJs6sn8J-IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Resampling with Aggregation\n",
        "\n",
        "Description: Aggregating data with a custom function during resampling. For example, calculating the sum or median during resampling.\n",
        "\n",
        "Example: Resampling daily data to weekly data and calculating the weekly sum."
      ],
      "metadata": {
        "id": "cPNWXnymKFKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resample daily data to weekly data and calculate the sum\n",
        "\n",
        "\n",
        "# plot original and resampled data\n"
      ],
      "metadata": {
        "id": "NaSNiF-fJ7OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "1.  Provides various aggregation options such as sum, mean, median, etc.\n",
        "2.  Useful for understanding data patterns over different time periods.\n",
        "\n",
        "Cons:\n",
        "\n",
        "1.  Aggregation can mask fluctuations.\n",
        "2.  Requires appropriate choice of aggregation method."
      ],
      "metadata": {
        "id": "Sbsi79W8KXN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Frequency Conversion\n",
        "\n",
        "Description: Converting time series data to a different frequency without resampling.\n",
        "\n",
        "Example: Converting data from daily frequency to business day frequency."
      ],
      "metadata": {
        "id": "wuMCVS2EKdCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate sample daily data\n",
        "\n",
        "\n",
        "# convert to business day frequency\n",
        "# 'B' for business day frequency\n",
        "\n",
        "# plot original and converted data\n"
      ],
      "metadata": {
        "id": "CYeJQCJ1KI5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "1.  Allows for conversion to different frequencies based on needs.\n",
        "2.  Can handle specific business-related requirements.\n",
        "\n",
        "Cons:\n",
        "\n",
        "1.  May result in missing values if frequency conversion does not align with original data.\n",
        "2.  Requires appropriate filling strategy for missing data."
      ],
      "metadata": {
        "id": "ytN1sFb-KlGn"
      }
    }
  ]
}