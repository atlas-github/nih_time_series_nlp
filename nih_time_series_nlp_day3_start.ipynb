{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMg/XMDBldZGTlDrguPjHa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/nih_time_series_nlp/blob/main/nih_time_series_nlp_day3_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09:00 am: Practical session 6"
      ],
      "metadata": {
        "id": "JVNTdblnVRiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Data into Training and Testing Sets for Time Series\n",
        "Since time series data has an inherent order, you typically split the data by preserving the time order (i.e., earlier data for training, later data for testing)."
      ],
      "metadata": {
        "id": "MxUxMtlQdXCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample time series data\n",
        "data = pd.date_range('2020-01-01', periods=100, freq='D')\n",
        "values = range(100)\n",
        "df = pd.DataFrame({'date': data, 'value': values})\n",
        "\n",
        "# Split the data by time index\n"
      ],
      "metadata": {
        "id": "UXkpfjXHFidT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time Series Cross-Validation Techniques\n",
        "The `TimeSeriesSplit` function in Scikit-learn can be used for time series cross-validation. It splits the data sequentially, preserving the order of time."
      ],
      "metadata": {
        "id": "Mq3P6uBHzj7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Sample data\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n"
      ],
      "metadata": {
        "id": "F9adtBX6zjic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rolling Window Cross-Validation\n",
        "This technique creates a series of rolling windows of training and testing data, which is suitable for time series forecasting tasks."
      ],
      "metadata": {
        "id": "CUDTT8RfzkXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Example with window size 80\n"
      ],
      "metadata": {
        "id": "jUEENb64zkeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09:45 am: Introduction to NLP data preprocessing"
      ],
      "metadata": {
        "id": "e-AX4rJsVSsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "Tokenize a sentence into words."
      ],
      "metadata": {
        "id": "zMPO-JpcVop5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "AyKc7ZVhdd3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQ7-3gUpVSkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopword removal\n",
        "\n",
        "Remove common stopwords from a list of tokens."
      ],
      "metadata": {
        "id": "frrtaV5NVpYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have downloaded stopwords: nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "uMHJwQeAVSx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "Apply stemming to reduce words to their base form."
      ],
      "metadata": {
        "id": "cKhOSbMEdOOS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtG1XSW6dOTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "Apply lemmatization to get the base form of words."
      ],
      "metadata": {
        "id": "kE4pAKaBdOYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have downloaded wordnet: nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "VsmyaN8rdOdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Normalization\n",
        "Convert text to lowercase and remove punctuation."
      ],
      "metadata": {
        "id": "ClpT5SIfdOhs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvpUzxXedOlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11:00 am: Text cleaning techniques"
      ],
      "metadata": {
        "id": "PB4xXitMVS2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Special Characters and Numbers\n",
        "Clean a text by removing special characters and numbers, keeping only letters and spaces."
      ],
      "metadata": {
        "id": "3-p0LzDWVqFI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efLm0MA-VS6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Case Sensitivity\n",
        "Convert all text to lowercase to handle case sensitivity."
      ],
      "metadata": {
        "id": "3E6VqiS5eQfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Be0Yf8tweQkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Stopwords\n",
        "Remove common stopwords from a text."
      ],
      "metadata": {
        "id": "xOaOGNaJeQou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have downloaded stopwords: nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "63Lb0-2LeQtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Punctuation\n",
        "Remove punctuation from a text."
      ],
      "metadata": {
        "id": "6WUL_MqoeQyF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNFFGy3peQ2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine techniques"
      ],
      "metadata": {
        "id": "ox5vmyRoeiOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have downloaded stopwords: nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "84Df07B3ej1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12:00 pm: Practical session 7"
      ],
      "metadata": {
        "id": "dhPRXns5VS-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install the gdown library\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "uSTNQtOHF47U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download dengue csv\n",
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "# Replace 'YOUR_FILE_ID' with the actual file ID\n",
        "file_id = '1F-faNnQoyhdjbuyEVZPHV1cv_h0h5UDl'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Download the CSV file\n",
        "gdown.download(url, 'dengue.csv', quiet=False)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df_dengue = pd.read_csv('dengue.csv')\n",
        "df_dengue"
      ],
      "metadata": {
        "id": "nTpXWvc9F49-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter to only needed columns\n",
        "df_dengue_filtered = df_dengue[[\"NO_KES\", \"NO_RUMAH\", \"POSKOD\", \"LOKALITI\", \"MUKIM\", \"DAERAH\", \"LATITUDE\", \"LONGITUDE\", \"STATUS_LOK\"]]\n",
        "\n",
        "df_dengue_filtered"
      ],
      "metadata": {
        "id": "o3zvaDDHxu65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine No_RUMAH and LOKALITI\n"
      ],
      "metadata": {
        "id": "5cudzFj6xu9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#which postcodes have most complete vs. most missing data?\n",
        "postcode_counts = df_dengue_filtered['POSKOD'].value_counts().reset_index()\n",
        "\n",
        "# Rename the columns\n",
        "postcode_counts.columns = ['postcode', 'count']\n",
        "\n",
        "postcode_counts"
      ],
      "metadata": {
        "id": "ZXOBE6FPxu_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how many NaNs in each column\n",
        "df_dengue_filtered.isna().sum()"
      ],
      "metadata": {
        "id": "8hRYakrjxvCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#filter to only missing postcodes\n"
      ],
      "metadata": {
        "id": "ebiOXQbnF5An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify similarity\n"
      ],
      "metadata": {
        "id": "qsp9cjeTx8n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install folium"
      ],
      "metadata": {
        "id": "ns0PEeY4yCjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "9uvonMJox8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(go.Scattermapbox(\n",
        "    lat=df_43100['LATITUDE'],\n",
        "    lon=df_43100['LONGITUDE'],\n",
        "    mode='markers',\n",
        "    marker=go.scattermapbox.Marker(size=9),\n",
        "    text=df_43100['LOKALITI']  # Display the place name when hovering\n",
        "))\n",
        "\n",
        "# Define the layout for the map\n",
        "fig.update_layout(\n",
        "    mapbox_style=\"open-street-map\",\n",
        "    mapbox=dict(\n",
        "        center=dict(lat=3.1390, lon=101.6869),  # Center the map\n",
        "        zoom=10\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the map\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "rNTkU4aFx8tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sign up on https://opencagedata.com/api\n"
      ],
      "metadata": {
        "id": "PijLpf7cyClm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyeSr8lgF5Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNxN6hslyUTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# form a dataframe to view latitudes and longitudes from opencagedata\n"
      ],
      "metadata": {
        "id": "8hjwlVRCyUVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOfj8fIlz01_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Open an account with [Google Cloud Platform](https://cloud.google.com/)\n",
        "2.   Pricing details are available [here](https://developers.google.com/maps/documentation/geocoding/usage-and-billing)\n",
        "3.   Enable Geocoding API, details [here](https://developers.google.com/maps/documentation/geocoding/start)\n",
        "\n"
      ],
      "metadata": {
        "id": "WIvfXWE6yhwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Replace YOUR_API_KEY with your actual Google API key\n"
      ],
      "metadata": {
        "id": "4bPQwnaByUX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r788TXtsyUaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# form dataframe to view oistcode, latitude, longitude, and formatted addresses\n"
      ],
      "metadata": {
        "id": "YyuzjnaxyUcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare data quality from opencagedata to Google Cloud Platform\n",
        "df_comparison = pd.merge(df_postcode_test, df_postcode_test_gcp, on=\"Addresses\", how=\"left\")\n",
        "df_comparison"
      ],
      "metadata": {
        "id": "tuebOVjIyUen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2:00 pm: Text normalization techniques"
      ],
      "metadata": {
        "id": "VSHLBvQBVTGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Text to Lowercase\n",
        "Convert all characters in a text to lowercase."
      ],
      "metadata": {
        "id": "hFYYKfKjVrfr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEuQuv9UVTLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanding Contractions\n",
        "Expand common contractions (e.g., \"don't\" to \"do not\")."
      ],
      "metadata": {
        "id": "91POS8aqeoc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "R7pyXBcne4EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlW9TPMgeoid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Special Characters and Numbers\n",
        "Remove special characters and numbers, keeping only letters and spaces."
      ],
      "metadata": {
        "id": "GhmYm6EaeonL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BtwYULBheosI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing Whitespace\n",
        "Normalize whitespace by collapsing multiple spaces into a single space and stripping leading/trailing spaces."
      ],
      "metadata": {
        "id": "6LaW1msNeowi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6gNmb3fOeo1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Non-ASCII Characters\n",
        "Remove non-ASCII characters from the text."
      ],
      "metadata": {
        "id": "zDC1DqeAfJ7G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeGlGEQTfKAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing Text Using Lemmatization\n",
        "Convert words to their base or dictionary form (lemmatization)."
      ],
      "metadata": {
        "id": "gd-ow-MDfKEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have downloaded wordnet: nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "h1Pfq69afKJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3:00 pm: Feature extraction for NLP"
      ],
      "metadata": {
        "id": "H2fShwHsVTO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (BoW)\n",
        "Convert a collection of text documents into a matrix of token counts."
      ],
      "metadata": {
        "id": "YBQihKHzVr79"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXIQnS_eVTTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "Convert text documents into a matrix of TF-IDF features."
      ],
      "metadata": {
        "id": "Tdknwuk1fcVY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tv7HjBMfcab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings (Word2Vec)\n",
        "Use pre-trained word embeddings to represent words in a text."
      ],
      "metadata": {
        "id": "HaaSmRXyfcf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"I love programming in Python.\",\n",
        "    \"Python programming is fun and exciting.\",\n",
        "    \"I enjoy solving complex problems using Python.\",\n",
        "    \"Natural Language Processing with Python is amazing.\",\n",
        "    \"Deep learning is a subset of machine learning.\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get the word embedding for the word 'python'\n",
        "python_vector = model.wv['python']\n",
        "print(f\"Embedding for 'python':\\n{python_vector}\")\n",
        "\n",
        "# Find the most similar words to 'python'\n",
        "similar_words = model.wv.most_similar('python', topn=3)\n",
        "print(\"\\nMost similar words to 'python':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ],
      "metadata": {
        "id": "dyq5GmpXfckF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Embeddings (Doc2Vec)\n",
        "Use Doc2Vec to obtain vector representations of entire documents."
      ],
      "metadata": {
        "id": "xBuZVs8vfcom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Prepare tagged documents\n",
        "documents = [\n",
        "    TaggedDocument(words=\"I love programming in Python\".lower().split(), tags=['doc1']),\n",
        "    TaggedDocument(words=\"Python programming is fun\".lower().split(), tags=['doc2']),\n",
        "    TaggedDocument(words=\"I enjoy solving problems with Python\".lower().split(), tags=['doc3'])\n",
        "]\n",
        "\n",
        "# Train a Doc2Vec model\n",
        "model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=4)\n",
        "model.build_vocab(documents)\n",
        "model.train(documents, total_examples=model.corpus_count, epochs=10)\n",
        "\n",
        "# Get document vectors\n",
        "doc1_vector = model.infer_vector([\"I\", \"love\", \"programming\", \"in\", \"python\"])\n",
        "print(doc1_vector)"
      ],
      "metadata": {
        "id": "S0F4gOfXfcsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams\n",
        "Extract n-grams (e.g., bigrams) from a text document."
      ],
      "metadata": {
        "id": "CHdibd4SgC3L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYcMG38WgC8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4:00 pm: Practical session 8"
      ],
      "metadata": {
        "id": "BTDO95TBVTYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try openstreetmap from https://nominatim.org/release-docs/develop/api/Search/#examples\n",
        "# https://nominatim.openstreetmap.org/search?q=Unter%20den%20Linden%201%20Berlin&format=json&addressdetails=1&limit=1&polygon_svg=1\n"
      ],
      "metadata": {
        "id": "l4m0RkaBGELA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url encode the address, i.e. no spaces or other special characters\n"
      ],
      "metadata": {
        "id": "gllee3H48hzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGbyd_yiSM9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmtlvpf-ANOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try another data source: open street map\n"
      ],
      "metadata": {
        "id": "Yg6_sepVqjnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5_bOl9mtZAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "in3LPEwSydnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqPWFnK4_hKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare results between opencagedata and openstreetmap\n"
      ],
      "metadata": {
        "id": "zu3IO5uYC0OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tG90rV6RDLAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#geocode for missing postcodes\n"
      ],
      "metadata": {
        "id": "MG24AIL4DYNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xO-ILFLnkCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJwc_DkjoA06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLAcPRyyoC9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# get postcodes, coordinates, and formatted addresses for other addresses\n"
      ],
      "metadata": {
        "id": "ZzkJ-0ounp__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-7DPmEhn548"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get list of tamans based on postcode\n"
      ],
      "metadata": {
        "id": "L8huB5zzyozc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vECWcRHkRQEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Your list of addresses\n",
        "addresses = [\n",
        "    \"3\\t37, Jalan USJ 6/5, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"4\\t8, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"5\\t18, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"6\\tN-00-001, Subang Perdana GoodYear Court, 2, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"8\\tN-00-001, Subang Perdana GoodYear Court, 2, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"9\\tUsj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"16\\t23, Jalan USJ 6/5, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"17\\tUsj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"19\\t11, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\"\n",
        "]\n",
        "\n",
        "# Prepare a list to hold all 2-grams\n",
        "two_grams = []\n",
        "\n",
        "# Define a function to extract 2-grams\n"
      ],
      "metadata": {
        "id": "xmAc3-bCR9ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Your list of addresses\n",
        "addresses = [\n",
        "    \"3\\t37, Jalan USJ 6/5, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"4\\t8, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"5\\t18, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"6\\tN-00-001, Subang Perdana GoodYear Court, 2, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"8\\tN-00-001, Subang Perdana GoodYear Court, 2, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"9\\tUsj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"16\\t23, Jalan USJ 6/5, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"17\\tUsj 6, 47610 Subang Jaya, Selangor, Malaysia\",\n",
        "    \"19\\t11, Jalan USJ 6/1, Usj 6, 47610 Subang Jaya, Selangor, Malaysia\"\n",
        "]\n",
        "\n",
        "# Function to extract n-grams\n",
        "def extract_ngrams(address, n):\n",
        "    # Tokenize the address into words, removing punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', address)\n",
        "    # Create n-grams from the list of words\n",
        "    return [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "# Set n for n-grams\n",
        "n = 3  # Change this value for different n-grams\n",
        "\n",
        "# Prepare a list to hold all n-grams\n",
        "n_grams = []\n",
        "\n",
        "# Extract n-grams from each address and accumulate them\n",
        "for address in list(df_postcode_test_gcp3[\"Addresses_for\"]):\n",
        "    n_grams.extend(extract_ngrams(address, n))\n",
        "\n",
        "# Count occurrences of each n-gram\n",
        "n_gram_counts = Counter(n_grams)\n",
        "\n",
        "# Get the most common n-gram\n",
        "most_common_ngram = n_gram_counts.most_common(1)\n",
        "\n",
        "# Print the most common n-gram\n",
        "if most_common_ngram:\n",
        "    n_gram, count = most_common_ngram[0]\n",
        "    print(f\"Most common {n}-gram: {' '.join(n_gram)}: {count}\")\n"
      ],
      "metadata": {
        "id": "rbmQiussSQPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58g-ggj0SheE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}